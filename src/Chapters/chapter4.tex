% !TEX root = ../main.tex
\chapter{Train the Deep Model for Prediction}\label{ch:chapter4} % For referencing the chapter elsewhere, use \ref{Chapter1}
%
%%----------------------------------------------------------------------------------------
The convolutional neural networks (CNNs) consist of neurons that have 
weights and bias, which can be trained using large datasets for solving 
computer vision tasks.
In this chapter, our architecture of
deep convolutional neural network for intra mode prediction 
is illustrated.
Then the hyper-parameters of our deep model is introduced
with explanations.
At the end of the chapter, the stopping criteria and training results 
are presented.
\begin{figure}
    \centering
    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{Figures/cnn_illustration.pdf}
    \caption[Two types of the residual units]{
        Two types of the residual units.}\label{fig:cnn-illustration}
\end{figure}

\section{The Architecture of CNN}\label{sec:cnn}
There exist many kinds of convolutional neural networks while the 
major difference that distinguish them from each other
is the uniqueness of each architecure.
Figure~\ref{fig:cnn-illustration} on 
page~\pageref{fig:cnn-illustration}
illustrates the basic architecture
of the convolutional neural networks.
The light blue cubics represent the Luma pixel values from a single
coding unit (CU) of size \(8\times8\) while the yellow cubics
shows a kernel of size \(3\times3\) that slides over the
CU in both directions.
At each position, the kernel does a weighted sum of all its inputs,
then adds biases. The output from the
region it covers will be fed into a neuron right 
below the covered region.
Although both are artificial neural networks, 
the big difference between multilayer perceptron and 
CNN is that weights of the neurons are shared in the latter case.
For example, all the neurons in Figure~\ref{fig:cnn-illustration} on
page~\pageref{fig:cnn-illustration} are reusing the same patch of
parameters, i.e., weights and biases.
For a single kernal of size \(3\times3\), it has 9 weights.
Apparently this amount of learnable parameters is just not enough.
More degrees of freedom are needed to enable 
the learning capability of a neural network.
For this reason, multiple kernels will be used instead of one single kernal.
If many kernels are stacked in a convolutional neural network such that
the architecture looks very deep, the network is called deep neural network
instead of simply neural network which typically refers to shallow ones.
In the deep convolutional neural networks, it can be imagined that
each convolutional layer comprising multiple kernels which subsequently produces
a three dimensional volume of the outputs.
After the convolutional layer, there normally exist activating layer where
the specified activation function will be applied to each single output
inside the output volume from the convolutional layer.
Followed by the activating layer, there is a pooling layer.
In the previous years, the maximum pooling and average pooling 
are popular methods for reducing the dimension of the outputs
from convolutional layers.
Nowadays, instead of applying a conventional pooling, 
a convolutional layer of larger stride is utilized.
Such a combination of convolution-activation-pooling will
be replicated multiple times for a single convolutional neural network.
In the tail of the network, a fully-connected layer can be used
to compute the probability of each target class for the 
classification problems.

Our network is built from the above description except the 
fact that we have adopted the identity mapping 
\(h(\mathbf{x}_l)=\mathbf{x}_l\) in the Residual 
Neural Network from~\parencite{RN67}.
It has been shown in~\parencite{RN68} that
the identity mapping in the residual units
can enable the direct propagation of information
from one layer to any other layers.
Such a nice property brings vital benefits
for deep neural networks such that the accuracy
saturation problem can be alleviated.
As a result, ultra-deep models are able to
learn desired representations for solving
problems.
Besides, with the identity mapping, the network
is able to converge faster hence the training time
required for a deep model can be reduced.
% The architecture of the deep 
Figure~\ref{fig:basic-resnet-structure} on 
page~\pageref{fig:basic-resnet-structure}
shows two types
of the residual units which implement the identity mapping
via shortcut connections from the beginning of the block
to the end of the same block.
\begin{figure}
    \centering
    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{Figures/basic-resnet-structure.pdf}
    \caption[Two types of the residual units]{
        Two types of the residual units are shown here.
        BN stands for Batch Normalization while
        Relu stands for Rectified Linear Unit.
        }\label{fig:basic-resnet-structure}
\end{figure}

The architecture of the deep convolutional neural 
network used for intra mode prediction in our work
is shown in Figure~\ref{fig:our-architecture} on
page~\pageref{fig:our-architecture}.
The first layer consists of 16 kernels where
the receptive fields are 
simply stacked on top of each other.
In the middle of the network, there are
three variants of resnet unit, each unit
is duplicated five times to deepen the network
for more learning capacities.
Batch Normalization is employed to regularize
the data inputs as well as the outputs from each layer.
The activation function is the most popular one named Relu.
In the end of the convolutional layers, global average pooling
has been performed before the fully-connected layer which 
tells the prediction results.


\begin{figure}
    \centering
    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{Figures/our-neural-net-structure.pdf}
    \caption[Top-20 precision on validation dataset for blocks of size \(4\times4\)]{
        Top-20 precision on validation dataset for blocks of size \(4\times4\).
        }\label{fig:our-architecture}
\end{figure}

\section{Settings of the Network}\label{sec:config}
In this section, the hyper-parameters that are
used for deep learning are given.
The configurations are fine-tuned results against the validation
datasets shown in Table~\ref{tab:finalized-eight-by-eight}
and Table~\ref{tab:finalized-sixteen-by-sixteen} to achieve the
best performance.

It is interesting to notice that inside this network, 
the size of any reception field
is always \(3\times3\) since it is found in~\parencite{RN62}
larger kernel can be factorized into smaller kernels.
A kernal with such a size is the smallest
field which is capable of capturing the positional
information~\parencite{RN107}.
All the strides are set to be one which means
no down-sampling will be performed. 
Hence the output size of each convolutional layer
will not be changed.
% The network configuration is shown in Table~\ref{tab:xxxx}.
For the inputs to the convolutional layers, 
zero padding (a.k.a\ SAME padding) is used.
It is a padding algorithm which has the objective
to pad zeros evenly for each direction.
No data augmentation is used for the inputs.
The kernal parameters are randomly initialized 
with a normal distribution where the mean 
is zero while the standard deviation is 
calculated using 
\(\sqrt{2\div(\mathit{a}\times\mathit{a}\times\mathit{n})}\).
Here the \(\mathit{a}\) represents the kernel size
while \(\mathit{n}\) denotes the number of filters
in the corresponding convolutional layer.
The weights of the kernels are regularized by
a decay of \(0.0002\).
No transfer learning is used which means all the learnable 
parameters are trained from scratch.
The batch size used for training is 128.
The loss function used is cross-entropy which has
a better performance for classification problems 
than other distance functions such as L1 and L2.
Stochastic gradient descent is used with a momentum 
of \(0.9\).
The learning rate decay policy is shown in 
Table~\ref{tab:lr-policy}.

\begin{table}[H]
    \caption{Learning rate policy}
    \bigskip\label{tab:lr-policy}
    \centering
    \begin{tabular}{c c}
        \toprule
        Global step & Learning rate \\
        \midrule
        \(<\)20,000 & 0.01 \\
        \(<\)40,000 & 0.001 \\
        \(<\)60,000 & 0.0001 \\
        Others  & 0.00001 \\
        \bottomrule
    \end{tabular}
\end{table}

\section{Stopping Criteria and Training Result}\label{sec:training}
During the training process, we keep eyes on 
various statistics to ensure the network is
truly learning high dimensional representations 
instead of being trapped
in an undesired status where nothing can be
learned.
Tensorflow~\parencite{tensorflow2015-whitepaper} has 
been used in this work to implement the 
deep convolutional neural network.
It has its own dedicated suite of visualization
tools named Tensorboard which is very helpful 
with understanding and monitoring the training process.
It has been configured to record the top-k precision
on the validation datasets during our training process.

Figure~\ref{fig:top20for4times4} 
on page~\pageref{fig:top20for4times4}
is a screen capture
of the Tensorboard user interface which contains the 
top-20 validation precision for blocks of size \(4\times4\).
It can be seem that top-20 precision cannot go 
beyond \(0.85\).
Figure~\ref{fig:top28for4times4}
on page~\pageref{fig:top28for4times4}
further shows the 
top-28 validation precision for blocks of the same size.
Although the highest precision can reach \(0.96\) 
in Figure~\ref{fig:top28for4times4}, it is not applicable
in our case since it involves 28 classes to achieve this 
acceptable precision.
Only four classes would be excluded.
Due to this observation, it is decided to
keep the intra mode decision process unmodified for 
blocks of size \(4\times4\) in \(HTM16.2\).

Figure~\ref{fig:top16for8times8}
on page~\pageref{fig:top16for8times8}
shows the 
top-16 validation precision for blocks of size \(8\times8\).
After 133k global steps, the top-16 precision curve in 
Figure~\ref{fig:top16for8times8} is very steady.
The precision value is higher than \(0.92\).
It means if this model can be employed to predict the 
most probable intra angular directions,
half of the angular directions can be skipped for
the Rate Distortion Cost evaluation process by which
a bunch of time shall be saved.
Figure~\ref{fig:top16for16times16} 
on page~\pageref{fig:top16for16times16}
shows the
top-16 validation precision for blocks 
of size \(16\times16\).
After 304k global steps, the top-16 precision in
Figure~\ref{fig:top16for16times16} has reached
\(0.9654\) which is considered as an excellent
result in our application scenario.
Once again, the half of the intra angular modes
can be bypassed if this model can be applied 
in \(HTM16.2\).
For the blocks of size \(32\times32\),
the dataset obtained for them does not 
have enough data
for the deep learning.
Instead of using a dedicated model for them,
the model trained from blocks of 
size \(16\times16\) has been reused.
The blocks of size \(32\times32\) are resized
to \(16\times16\) using Bilinear Interpolation.

Apart from observing the validation precision,
confusion matrix is the other useful criteria 
to help making decision on when to stop the 
training.
Figure~\ref{fig:cm8times8}
on page~\pageref{fig:cm8times8} shows the confusion
matrix after 173 epochs of training for blocks
of size \(8\times8\).
Figure~\ref{fig:cm16times16}
on page~\pageref{fig:cm16times16} shows the confusion
matrix after 396 epochs of training for blocks
of size \(16\times16\).
Both of the two confusion matrices exhibit
good property which shows most of the predictions
fall in the top-k predictions.

\begin{figure}
    \begin{minipage}{0.98\textwidth}
    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{Figures/blk-4--top-20.png}
    \caption[Top-20 precision on validation dataset for blocks of size \(4\times4\)]{
        Top-20 precision on validation dataset for blocks of size \(4\times4\).
        }\label{fig:top20for4times4}
    \end{minipage}
    
    \vspace*{1cm} % vertical separation

    \begin{minipage}{0.98\textwidth}
    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{Figures/blk-4--top-28.png}
    \caption[Top-28 precision on validation dataset for blocks of size \(4\times4\)]{
        Top-28 precision on validation dataset for blocks of size \(4\times4\).
        }\label{fig:top28for4times4}
    \end{minipage}
\end{figure}

\begin{figure}
    \begin{minipage}{0.98\textwidth}
    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{Figures/blk-8--top-16.png}
    \caption[Top-16 precision on validation dataset for blocks of size \(8\times8\)]{
        Top-16 precision on validation dataset for blocks of size \(8\times8\).
        }\label{fig:top16for8times8}
    \end{minipage}
    
    \vspace*{1cm} % vertical separation

    \begin{minipage}{0.98\textwidth}
    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{Figures/blk16-top16.png}
    \caption[Top-16 precision on validation dataset for blocks of size \(16\times16\)]{
        Top-16 precision on validation dataset for blocks of size \(16\times16\).
        }\label{fig:top16for16times16}
    \end{minipage}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{Figures/confusion-matrix/08-kpt-133049.eps}
    \caption[Confusion matrix obtained after 133049 global steps of model training for blocks of size \(8\times8\)]
    {Confusion matrix obtained after 133049 global steps of model training for blocks of size \(8\times8\)}\label{fig:cm8times8}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{Figures/confusion-matrix/16-kpt-304857.eps}
    \caption[Top-k precision on testing datasets for each block size]
    {Top-k precision on testing datasets for each block size}\label{fig:cm16times16}
\end{figure}

\section{Evaluate the Learned Models}\label{sec:eval-section}
Two models have been obtained after the training process.
One of the models is is trained from blocks of size \(8\times8\) and 
applied for blocks with the same size.
The other one is trained from blocks of size \(16\times16\), however, 
it is applied for blocks of size \(16\times16\) and size \(32\times32\).
The top-k precision on testing datasets for various 
block sizes is reported in
Table~\ref{tab:valprecision} 
on page~\pageref{tab:valprecision}
(see also Figure~\ref{fig:topkPrecisions}
on page~\pageref{fig:topkPrecisions}
).
Notice that the maximum size for DMM1 wedgelet prediction 
is \(32\times32\), hence there is no need to perform evaluation 
for blocks of size \(64\times64\).
To evaluate performance on blocks of size \(32\times32\),
four resizing methods, i.e., Bilinear Interpolation, 
Nearest Neighbor Interpolation, 
Bicubic Interpolation, and
Area Interpolation,
have been tried.
It turns out the differences among 
the four resizing methods are so trivial
in terms of the prediction accuracies.
Here Bilinear Interpolation
is chosen to be applied in our work.

It can be observed that starting from top-14,
the error rates are all below \(10\% \).
And the model learns well for larger block sizes.
When the models are integrated to the encoder,
if a larger value of \(\mathbf{k}\) is used, 
fewer modes can be excluded which can 
lead to a small time reduction in the encoding process.
In the contrary, if smaller value of \(\mathbf{k}\) is used,
the prediction accuracy may not be high enough
to ensure the encoder performance.
In our work top-15 has been adopted which 
is responsible of balancing
the trade-off between coding performance and 
the time cost in the encoder.



\begin{table}
    \caption{Top-k precision on testing datasets for each block size}
    \bigskip\label{tab:valprecision}
    \centering
    \begin{tabular}{c c c c c}
        \toprule
        \# & Top-k & Size \(8\times8\) & Size \(16\times16\) & Size \(32\times32\) \\
        \midrule
        1 & Top-5 & 0.650     & 0.801  & 0.819 \\
        2 & Top-6  & 0.711    & 0.842  & 0.860 \\
        3 & Top-7  & 0.759    & 0.873  & 0.891 \\
        4 & Top-8  & 0.795    & 0.895  & 0.912 \\
        5 & Top-9  & 0.823    & 0.912  & 0.927 \\
        6 & Top-10  & 0.846   & 0.924   & 0.938 \\
        7 & Top-11  & 0.867   & 0.934   & 0.947 \\
        8 & Top-12  & 0.884   & 0.942   & 0.955 \\
        9 & Top-13  & 0.897   & 0.949   & 0.961 \\
        10 & Top-14  & 0.909  & 0.955   & 0.966 \\
        11 & Top-15  & 0.919  & 0.961   & 0.970 \\
        12 & Top-16  & 0.929  & 0.965   & 0.973 \\
        13 & Top-17  & 0.936  & 0.970   & 0.977 \\
        14 & Top-18  & 0.944  & 0.974   & 0.980 \\
        15 & Top-19  & 0.950  & 0.977   & 0.983 \\
        16 & Top-20  & 0.955  & 0.980   & 0.985 \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{Figures/topkPrecisions.pdf}
    \caption[Top-k precision on testing datasets for each block size]
    {Top-k precision on testing datasets for each block size}\label{fig:topkPrecisions}
\end{figure}
