% !TEX root = ../main.tex
\chapter{Train the Deep Model for Prediction}\label{ch:chapter4} % For referencing the chapter elsewhere, use \ref{Chapter1}
%
%%----------------------------------------------------------------------------------------
The convolutional neural networks (CNNs) consist of neurons that have 
weights and bias, which can be trained using large datasets for solving 
computer vision tasks.
In this chapter, our architecture of
deep convolutional neural network for intra mode prediction 
is illustrated.
Then the hyper-parameters of our deep model is introduced.
At the end of the chapter, the stopping criteria and training results 
are presented.
\begin{figure}
    \centering
    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{Figures/cnn_illustration.pdf}
    \caption[The basic unit of the convolutional neural networks]{
        The basic unit of the convolutional neural networks.}\label{fig:cnn-illustration}
\end{figure}

\section{The Architecture of CNN}\label{sec:cnn}
There exist many kinds of convolutional neural networks while the 
major difference that distinguish them from each other
is the uniqueness of each architecure.
Figure~\ref{fig:cnn-illustration} on 
page~\pageref{fig:cnn-illustration}
illustrates the basic unit
of convolutional neural networks.
The light blue cubics represent the Luma pixel values from a single
coding unit (CU) of size \(8\times8\).
The yellow cubics show a kernel of size \(3\times3\) that 
slides over the
CU vertically and horizontally.
At each position, the kernel calculates a weighted sum of all its inputs,
then adds a bias.
The output from the
region it covers will be fed into a neuron right
below the covered region.
% Although both are artificial neural networks, 
There is a big difference between multilayer perceptron and 
CNN\: the weights of the neurons are shared in the latter case.
For example, all the neurons in Figure~\ref{fig:cnn-illustration} on
page~\pageref{fig:cnn-illustration} are reusing the same patch of
parameters, i.e., weights and biases.
For a single kernal of size \(3\times3\), it has 9 weights.
Apparently this amount of learnable parameters is just not enough
to learn the representations.
More degrees of freedom are needed to enhance
the learning capacity of the neural network.
For this reason, a large number of kernels will be used 
instead of one single kernal.
If many kernels are stacked in a convolutional neural network such that
the architecture looks very deep, the network is called deep neural network
instead of simply neural network which typically refers to shallow ones.
In deep convolutional neural network,
each convolutional layer comprising multiple kernels which subsequently produces
a three dimensional volume of the outputs.
After the convolutional layer, there normally exist an activating layer where
the specified activation function will be applied to each single output
inside the output volume from the convolutional layer.
Followed by the activating layer, there is a pooling layer.
In the previous years, the maximum pooling and average pooling 
are popular methods for reducing the dimension of the outputs
from convolutional layers.
Nowadays, instead of applying a conventional pooling, 
a convolutional layer of stride \(> 1\) is utilized.
Such a combination of convolution-activation-pooling will
be replicated as many times as you wish for a 
single convolutional neural network.
In the tail of the network, a fully-connected layer will be used
to compute the probability of each target class for the 
classification problems.

Our network is built with above description as guideline except the 
fact that we have adopted the identity mapping 
\(h(\mathbf{x}_l)=\mathbf{x}_l\) in the Residual 
Neural Network from~\parencite{RN67}.
It is shown in~\parencite{RN68} that
the identity mapping in residual units
can enable the direct propagation of information
from one layer to any other layers.
Such a nice property brings vital benefits
for deep neural networks such that the saturation of accuracy
can be largely alleviated.
As a result, ultra-deep models are able to
learn better representations to solve
vision tasks.
Besides, with the identity mapping, the network
is able to converge faster hence the training time
required to train a deep model can be reduced.
% The architecture of the deep 
Figure~\ref{fig:basic-resnet-structure} on 
page~\pageref{fig:basic-resnet-structure}
shows two types
of the residual units in which the identity mapping
is implemented by shortcut connections from 
the beginning of the block
to the end of the same block.

The architecture of the deep convolutional neural 
network used for intra mode prediction in our work
is shown in Figure~\ref{fig:our-architecture} on
page~\pageref{fig:our-architecture}.
The first layer consists of 16 kernels where
the receptive fields are 
simply stacked on top of each other.
In the middle of the network, there are
three variants of resnet unit, each unit
is replicated five times to deepen the network
for increasing learning capacities.
Batch Normalization is employed to regularize
data inputs to each of the layers.
The activation function is the most popular one named Relu.
After all the convolutional layers, global average pooling
has been performed before the fully-connected layer.
For each of the neurons in the fully-connected layer,
the softmax function is applied to calculate 
the probability which tells how likely
the class corresponding to that neuron is the truth.
The softmax equation is
\begin{equation}
    Y_m = softmax(L_m) = \displaystyle\frac{e^{L_m}}{\displaystyle\sum_{a}^{A} e^{L_a}}
\end{equation}
% {n=1}^{\infty}
where \(A\) denotes the total number of classes, and \(L_m\) is the weighted sum of all related pixels adding a bias:
\begin{equation}
    L_m = X_m\cdot W_m + b
\end{equation}
% where the \(\mathbf{X}\) denotes a 2D matrix
% of input data, \(\mathbf{W}\) denotes the weights matrix
% and \(b\) is the bias.
During training, the output \(Y_i\) from softmax layer
will be fed into the cross-entropy loss function:
\begin{equation}
    \label{eq:entropy}
    E_{\mbox{\tiny xent}} = -\sum_i T_i \log Y_i
\end{equation}
where \(T_i\) is the true label of each class.
% The cross-entropy loss function will be utilized in
% each of the training steps
The learning happens when each of the training steps 
tries to minimize the cross entropy loss function
using backpropagation.

In the backpropagation, we first calculate the
gradient for weights in the last layer (a.k.a\ top layer):
% \begin{equation}
\begin{align}
    \frac{\partial E_{\mbox{\tiny xent}}}{\partial W_{nm}} &= \frac{\partial E_{\mbox{\tiny xent}}}{\partial L_m} \cdot \frac{\partial L_m}{\partial W_{nm}}\\
    &= \sum_{r}^{A} \frac{\partial E_{\mbox{\tiny xent}}}{\partial Y_r} \cdot \frac{\partial Y_r}{\partial L_m} \cdot X_m\\
    &= (\frac{\partial E_{\mbox{\tiny xent}}}
            {\partial Y_m} 
        \cdot 
        \frac{\partial Y_m}
        {\partial L_m} 
        + 
        \sum_{m\neq r}
        \frac{\partial E_{\mbox{\tiny xent}}}
        {\partial Y_r} 
        \cdot 
        \frac{\partial Y_r}{\partial L_m} )\cdot X_m\\
    &= (- \frac{T_m}{Y_m} \cdot Y_m \cdot (1-Y_m) + \sum_{m\neq r}\frac{-T_r}{Y_r}\cdot(-Y_r\cdot Y_m)) \cdot X_m\\
    &= (-T_m\cdot(1-Y_m) + \sum_{m\neq r} T_r\cdot Y_m) \cdot X_m\\
    &= (-T_m + T_m\cdot\ Y_m + \sum_{m\neq r} T_r\cdot Y_m) \cdot X_m\\
    &= (-T_m + \sum_r T_r\cdot Y_m) \cdot X_m\\
    &= (-T_m + Y_m \sum_r T_r) \cdot X_m\\
    &= (-T_m + Y_m) \cdot X_n
\end{align}
% \end{equation}
At this time, a fraction of the gradient will be used 
to update the weights and biases
in the network.
Moreover, For neurons in the hidden layer \(p\):
\begin{align}
    \frac{\partial E_{\mbox{\tiny xent}}}{\partial L_p} &= \frac{\partial E_{\mbox{\tiny xent}}}{\partial L_m} \cdot \frac{\partial L_m}{\partial X_p} \cdot \frac{\partial X_p}{\partial L_p}\\
    &= (Y_m - T_m)\cdot W_{pm} \cdot (X_p\cdot(1-X_p))
\end{align}
The rest gradient calculations can be derived in the similar manner.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{Figures/basic-resnet-structure.pdf}
    \caption[Two types of the residual units]{
        Two types of the residual units are shown here.
        BN stands for Batch Normalization while
        Relu stands for Rectified Linear Unit.
        }\label{fig:basic-resnet-structure}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{Figures/our-neural-net-structure.pdf}
    \caption[The architecture of the deep convolutional neural 
    network used for intra mode prediction in our work]{
        The architecture of the deep convolutional neural 
        network used for intra mode prediction in our work.
        }\label{fig:our-architecture}
\end{figure}

\section{Settings of the Network}\label{sec:config}
In this section, the hyper-parameters, which have been
used for deep learning, are given.
The configurations are fine-tuned results against the validation
datasets shown in Table~\ref{tab:finalized-eight-by-eight}
and Table~\ref{tab:finalized-sixteen-by-sixteen} to achieve the
best performance.

It is interesting to notice that in this network, 
the size of any reception field
is always \(3\times3\).
It is found in~\parencite{RN62} that
larger kernel can be factorized into smaller kernels.
A kernal with such a size is the smallest
field which is capable of capturing the positional
information~\parencite{RN107}.
All the strides are set to be one, which means
no down-sampling will be performed.
The depth CU,
which can be classified into DMM1 mode, is essentially
a feature itself if it is taken as a whole.
Therefore there is no need to down-sampling
the CU into smaller sizes.
Hence the output size of each convolutional layer
will not be changed.
% The network configuration is shown in Table~\ref{tab:xxxx}.
For the inputs to the convolutional layers, 
zero padding (a.k.a\ SAME padding) is used.
It is a padding algorithm which has the objective
to pad zeros evenly for each direction.
No data augmentation is used for the inputs.
The kernal parameters are randomly initialized 
with a normal distribution where the mean 
is zero while the standard deviation is 
calculated using 
\(\sqrt{2\div(\mathit{a}\times\mathit{a}\times\mathit{n})}\).
Here the \(\mathit{a}\) represents the kernel size
while \(\mathit{n}\) denotes the number of filters
in the corresponding convolutional layer.
The weights of the kernels are regularized by
a decay of \(0.0002\).
No transfer learning is used, which means all the learnable 
parameters are trained from scratch.
The batch size used for training is 128.
The loss function used is cross-entropy which has
a better performance for classification problems 
than other distance functions such as L1 and L2.
Stochastic gradient descent is used with a momentum 
of \(0.9\).
The learning rate decay policy is shown in 
Table~\ref{tab:lr-policy}.

\begin{table}[H]
    \caption{Learning rate policy}
    \bigskip\label{tab:lr-policy}
    \centering
    \begin{tabular}{c c}
        \toprule
        Global step & Learning rate \\
        \midrule
        \(<\)20,000 & 0.01 \\
        \(<\)40,000 & 0.001 \\
        \(<\)60,000 & 0.0001 \\
        Others  & 0.00001 \\
        \bottomrule
    \end{tabular}
\end{table}

\section{Stopping Criteria and Training Result}\label{sec:training}
During training, we keep eyes on 
various statistics to ensure the network is
truly learning high dimensional representations 
instead of being trapped
in a undesired status where nothing can be
learned.
Tensorflow~\parencite{tensorflow2015-whitepaper} has 
been used in this work to implement the 
deep convolutional neural network.
It has its own dedicated suite of visualization
tools named Tensorboard which is very helpful 
with understanding and monitoring the training process.
It has been configured to record the top-k precision
on the validation datasets during training.

Figure~\ref{fig:top20for4times4} 
on page~\pageref{fig:top20for4times4}
is a screen capture
of the Tensorboard user interface which contains the 
top-20 validation precision for blocks of size \(4\times4\).
It can be seem that top-20 precision cannot go 
beyond \(0.85\).
Figure~\ref{fig:top28for4times4}
on page~\pageref{fig:top28for4times4}
further shows the 
top-28 validation precision for blocks of the same size.
Although the highest precision can reach \(0.96\) 
in Figure~\ref{fig:top28for4times4}, it is not applicable
in our case since it involves 28 classes to achieve this 
acceptable precision.
Only four classes would be excluded.
Due to this observation, it is decided to
keep the intra mode decision process unmodified for 
blocks of size \(4\times4\) in \(HTM16.2\).

Figure~\ref{fig:top16for8times8}
on page~\pageref{fig:top16for8times8}
shows the 
top-16 validation precision for blocks of size \(8\times8\).
After 133k global steps, the top-16 precision curve in 
Figure~\ref{fig:top16for8times8} is very steady.
The precision value is higher than \(0.92\).
It means if this model can be employed to predict the 
most probable intra angular directions,
half of the angular directions can be skipped for
the Rate Distortion Cost evaluation process by which
a bunch of time shall be saved.
Figure~\ref{fig:top16for16times16} 
on page~\pageref{fig:top16for16times16}
shows the
top-16 validation precision for blocks 
of size \(16\times16\).
After 304k global steps, the top-16 precision in
Figure~\ref{fig:top16for16times16} has reached
\(0.9654\) which is considered as an excellent
result in our application scenario.
Once again, the half of the intra angular modes
can be bypassed if this model can be applied 
in \(HTM16.2\).
For the blocks of size \(32\times32\),
the dataset obtained for them does not 
have enough data
for the deep learning.
Instead of using a dedicated model for them,
the model trained from blocks of 
size \(16\times16\) has been reused.
The blocks of size \(32\times32\) are resized
to \(16\times16\) using Bilinear Interpolation.

Apart from monitoring validation precision,
confusion matrix is the other useful criteria 
to help making decision on when to stop the 
training.
Figure~\ref{fig:cm8times8}
on page~\pageref{fig:cm8times8} shows the confusion
matrix after 173 epochs of training for blocks
of size \(8\times8\).
Figure~\ref{fig:cm16times16}
on page~\pageref{fig:cm16times16} shows the confusion
matrix after 396 epochs of training for blocks
of size \(16\times16\).
Both of the two confusion matrices exhibit
good property which shows most of the predictions
fall in the top-k predictions.

\begin{figure}
    \begin{minipage}{0.98\textwidth}
    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{Figures/blk-4--top-20.png}
    \caption[Top-20 precision on validation dataset for blocks of size \(4\times4\)]{
        Top-20 precision on validation dataset for blocks of size \(4\times4\).
        }\label{fig:top20for4times4}
    \end{minipage}
    
    \vspace*{1cm} % vertical separation

    \begin{minipage}{0.98\textwidth}
    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{Figures/blk-4--top-28.png}
    \caption[Top-28 precision on validation dataset for blocks of size \(4\times4\)]{
        Top-28 precision on validation dataset for blocks of size \(4\times4\).
        }\label{fig:top28for4times4}
    \end{minipage}
\end{figure}

\begin{figure}
    \begin{minipage}{0.98\textwidth}
    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{Figures/blk-8--top-16.png}
    \caption[Top-16 precision on validation dataset for blocks of size \(8\times8\)]{
        Top-16 precision on validation dataset for blocks of size \(8\times8\).
        }\label{fig:top16for8times8}
    \end{minipage}
    
    \vspace*{1cm} % vertical separation

    \begin{minipage}{0.98\textwidth}
    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{Figures/blk16-top16.png}
    \caption[Top-16 precision on validation dataset for blocks of size \(16\times16\)]{
        Top-16 precision on validation dataset for blocks of size \(16\times16\).
        }\label{fig:top16for16times16}
    \end{minipage}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{Figures/confusion-matrix/08-kpt-133049.eps}
    \caption[Confusion matrix after 173 epochs of model training for blocks of size \(8\times8\)]
    {Confusion matrix after 173 epochs of model training for blocks of size \(8\times8\).}\label{fig:cm8times8}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{Figures/confusion-matrix/16-kpt-304857.eps}
    \caption[Confusion matrix after 396 epochs of training for blocks of size \(16\times16\)]
    {Confusion matrix after 396 epochs of training for blocks of size \(16\times16\).}\label{fig:cm16times16}
\end{figure}

\section{Evaluate the Learned Models}\label{sec:eval-section}
Two models have been obtained after the training process.
One of the models is trained from blocks of size \(8\times8\) and 
is applied to blocks with the same size.
The other one is trained from blocks of size \(16\times16\), however, 
it is applied to blocks of size \(16\times16\) and size \(32\times32\).
The top-k precision on testing datasets for various 
block sizes is reported in
Table~\ref{tab:valprecision} 
on page~\pageref{tab:valprecision}
(see also Figure~\ref{fig:topkPrecisions}
on page~\pageref{fig:topkPrecisions}
).
Notice that the maximum size for DMM1 wedgelet prediction 
is \(32\times32\), hence there is no need to perform evaluation 
for blocks of size \(64\times64\).
To evaluate performance on blocks of size \(32\times32\),
four resizing methods, i.e., Bilinear Interpolation, 
Nearest Neighbor Interpolation, 
Bicubic Interpolation, and
Area Interpolation,
have been tried.
It turns out the differences among 
the four resizing methods are so trivial
in terms of the prediction accuracy.
Here Bilinear Interpolation
has been chosen to be applied in our work.
It can be observed that starting from top-14,
the error rates are all below \(10\% \).
And the model learns well for larger block sizes.
When the models are integrated to the encoder,
if a larger value of \(\mathbf{k}\) is used, 
fewer modes can be excluded which can 
result in a small time reduction in the encoding process.
In the contrary, if smaller value of \(\mathbf{k}\) is used,
the prediction accuracy may not be high enough
to ensure the encoder performance.
In our work top-15 has been adopted which 
is responsible for balancing
the trade-off between coding performance and 
the time cost in the encoder.



\begin{table}
    \caption{Top-k precision on testing datasets for various 
    block sizes}
    \bigskip\label{tab:valprecision}
    \centering
    \begin{tabular}{c c c c c}
        \toprule
        \# & Top-k & Size \(8\times8\) & Size \(16\times16\) & Size \(32\times32\) \\
        \midrule
        1 & Top-5 & 0.650     & 0.801  & 0.819 \\
        2 & Top-6  & 0.711    & 0.842  & 0.860 \\
        3 & Top-7  & 0.759    & 0.873  & 0.891 \\
        4 & Top-8  & 0.795    & 0.895  & 0.912 \\
        5 & Top-9  & 0.823    & 0.912  & 0.927 \\
        6 & Top-10  & 0.846   & 0.924   & 0.938 \\
        7 & Top-11  & 0.867   & 0.934   & 0.947 \\
        8 & Top-12  & 0.884   & 0.942   & 0.955 \\
        9 & Top-13  & 0.897   & 0.949   & 0.961 \\
        10 & Top-14  & 0.909  & 0.955   & 0.966 \\
        11 & Top-15  & 0.919  & 0.961   & 0.970 \\
        12 & Top-16  & 0.929  & 0.965   & 0.973 \\
        13 & Top-17  & 0.936  & 0.970   & 0.977 \\
        14 & Top-18  & 0.944  & 0.974   & 0.980 \\
        15 & Top-19  & 0.950  & 0.977   & 0.983 \\
        16 & Top-20  & 0.955  & 0.980   & 0.985 \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{Figures/topkPrecisions.pdf}
    \caption[Top-k precision on testing datasets for various 
    block sizes]
    {Top-k precision on testing datasets for various 
    block sizes.}\label{fig:topkPrecisions}
\end{figure}
